{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Projeto Final - Noções de Inteligência Artificial 2025/1**\n",
        "## **Alunos**:\n",
        "- Caio Medeiros Balaniuk (matrícula: 231025190)\n",
        "- Davi Henrique Vieira Lima (matrícula: 231013529)\n",
        "- Gabriel Caixeta Romero (matrícula: 232036896)\n",
        "- Vitor Amorim Mello (matrícula: 231037048)\n"
      ],
      "metadata": {
        "id": "E44y2rvRGzkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Instalação da Biblioteca D2L**\n",
        "A biblioteca `D2L` (**Dive Into Deep Learning**) é necessária ser instalada para permitir:\n",
        "  *  Utilitários prontos para carregar e visualizar datasets como MNIST, Fashion-MNIST, entre outros.\n",
        "  *  Funções auxiliares para construir, treinar e avaliar redes neurais em PyTorch.\n",
        "  *  Classes úteis como `DataModule`, `Trainer`, `Classifier`, entre outras, que facilitam o desenvolvimento e experimentação com redes neurais."
      ],
      "metadata": {
        "id": "NjWJq7FpJxyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ITaVSokQJzFF",
        "outputId": "299375e6-c4a4-4057-e8ca-38ee3bf1cc13"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: d2l in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.11/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (from d2l) (1.23.5)\n",
            "Requirement already satisfied: matplotlib==3.7.2 in /usr/local/lib/python3.11/dist-packages (from d2l) (3.7.2)\n",
            "Requirement already satisfied: matplotlib-inline==0.1.6 in /usr/local/lib/python3.11/dist-packages (from d2l) (0.1.6)\n",
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.11/dist-packages (from d2l) (2.31.0)\n",
            "Requirement already satisfied: pandas==2.0.3 in /usr/local/lib/python3.11/dist-packages (from d2l) (2.0.3)\n",
            "Requirement already satisfied: scipy==1.10.1 in /usr/local/lib/python3.11/dist-packages (from d2l) (1.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (6.5.7)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->d2l) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (11.2.1)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.2->d2l) (2.9.0.post0)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from matplotlib-inline==0.1.6->d2l) (5.7.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3->d2l) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.0.3->d2l) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests==2.31.0->d2l) (2025.6.15)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.2->d2l) (1.17.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1.12)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.4.2)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.10)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.15)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.19.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.13.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.6)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.8.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (25.1.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.3.1)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.4.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter==1.0.0->d2l) (1.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (4.9.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l) (4.3.8)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (4.24.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.13)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (4.14.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter==1.0.0->d2l) (0.26.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter==1.0.0->d2l) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Importação de Bibliotecas**\n",
        "Para permitir o desenvolvimento do trabalho, é necessário a importação das seguintes bibliotecas:\n",
        "  * `pandas`: principal biblioteca para **manipulação e análise de dados tabulares** (DataFrames).\n",
        "  * `os`: biblioteca para **interagir com o sistema operacional**, útil para manipulação de arquivos e diretórios.\n",
        "  * `gdown`: biblioteca utilizada para **fazer download de arquivos diretamente do Google Drive**, facilitando o carregamento de datasets.\n",
        "  * `zipfile`: módulo padrão do Python para **descompactar arquivos .zip**, usado quando o dataset vem comprimido.\n",
        "  * `torch`: principal biblioteca de **machine learning com PyTorch**.\n",
        "  * `torch from d2l`: apresenta utilitários da biblioteca **Dive into Deep Learning (d2l)** para PyTorch.\n",
        "  * `nn from torch`: módulo de **redes neurais (neural networks)** do PyTorch.\n",
        "  * `train_test_split from sklearn.model_selection`: função para **dividir os dados em conjuntos de treino e teste**, importante para validação do modelo.\n",
        "  * `StandardScaler from sklearn.preprocessing`: usada para **normalizar características numéricas**, garantindo que cada feature tenha média 0 e desvio padrão 1.\n",
        "  * `matplotlib.pyplot`: permite a visualização gráfica do **Matplotlib**."
      ],
      "metadata": {
        "id": "gUFFOgMcJ2Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gdown\n",
        "import zipfile\n",
        "import torch\n",
        "from d2l import torch as d2l\n",
        "from torch import nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-9s3VKoNJ7_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Dataset**"
      ],
      "metadata": {
        "id": "IRUJqdLzLUVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.1. Tabela FIPE**\n",
        "O dataset utilizado neste trabalho é composto por dados extraídos da **Tabela Fipe**, que é uma referência oficial no Brasil para a **avaliação de preços médios de veículos automotores** (carros, motos e caminhões).\n",
        "\n",
        "A **Tabela Fipe** é elaborada pela **Fundação Instituto de Pesquisas Econômicas (FIPE)** e fornece mensalmente o **preço médio praticado no mercado nacional para veículos novos e usados**.\n",
        "\n",
        "Cada registro da Tabela Fipe geralmente contém informações como:\n",
        "* **Marca do veículo** (ex: Fiat, Honda, Toyota),\n",
        "\n",
        "* **Modelo do veículo** (ex: Palio 1.0, Civic EXL),\n",
        "\n",
        "* **Ano de fabricação** (anomod),\n",
        "\n",
        "* **Combustível utilizado** (ex: Gasolina, Flex, Diesel),\n",
        "\n",
        "* **Mês/Ano de referência da cotação** (mesref / anoref),\n",
        "\n",
        "* **Valor de mercado estimado** (em reais)."
      ],
      "metadata": {
        "id": "OEkOg8rkTG8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.2. Extração e Organização do Dataset**\n",
        "O conjunto de dados foi gerado utilizando um **web crawler** disponível no repositório [fipe-crawler](https://github.com/rafaelgou/fipe-crawler.git), que automatiza a coleta da Tabela Fipe diretamente do site oficial.\n",
        "\n",
        "Foram coletadas informações dos **últimos 12 meses**, organizadas em múltiplos arquivos CSV (um para cada mês), contendo milhares de registros.\n",
        "\n",
        "Esses arquivos foram agrupados em um **arquivo `.zip` hospedado no Google Drive**, sendo baixados e descompactados automaticamente pelo seguinte trecho de código:"
      ],
      "metadata": {
        "id": "AMqisMTET-uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar o arquivo zip do Google Drive\n",
        "url = \"https://drive.google.com/uc?id=125NtHsqQaokd76-ThXVmQLvLy1vXVd56\"\n",
        "output = \"fipe_dataset.zip\"\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Descompactar\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"fipe_dataset_extracted\")\n",
        "\n",
        "# Caminho da pasta\n",
        "caminho_pasta = \"/content/fipe_dataset_extracted/dataset\"\n"
      ],
      "metadata": {
        "id": "rx3-iYgFtXGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.3. Leitura e Pré-processamento Inicial dos Dados**\n",
        "\n",
        "Nesta etapa, realizamos a leitura dos arquivos `.csv` extraídos do arquivo `.zip` contendo os dados mensais da Tabela Fipe.\n",
        "\n",
        "Etapas do processo:\n",
        "\n",
        "1. **Listagem dos Arquivos:** Todos os arquivos dentro do diretório da base são listados e ordenados alfabeticamente para garantir consistência na leitura.\n",
        "\n",
        "2. **Leitura de Cada Arquivo CSV:** Cada arquivo `.csv` é lido individualmente utilizando o `pandas.read_csv`. Caso haja erro na leitura de algum arquivo (ex: linhas mal formatadas), ele será ignorado com a opção `on_bad_lines=\"skip\"`.\n",
        "\n",
        "3. **Concatenação dos Dados:** Todos os DataFrames lidos são concatenados em um único DataFrame chamado `dados`, unificando todos os registros coletados ao longo dos meses.\n",
        "\n",
        "4. **Criação da Coluna `timeprod`:** Essa coluna representa o tempo de produção do veículo (em meses) desde seu ano de fabricação (`anomod`) até o ano e mês de referência (`anoref`, `mesref`).\n",
        "\n",
        "5. **Seleção de Colunas Relevantes:** Apenas as colunas de interesse são mantidas para a análise: marca, modelo, ano de fabricação, tipo de combustível, tempo de produção e valor.\n",
        "\n",
        "6. **Filtragem de Registros com Valores Realistas:** São removidos registros com anos ou tempos de produção considerados fora do intervalo aceitável (anomalias ou erros de coleta).\n",
        "\n",
        "7. **Remoção de Duplicatas:** Garante que não haja linhas repetidas no conjunto final.\n",
        "\n",
        "8. **Exibição Final:** Apresenta o DataFrame resultante com os dados prontos para o pré-processamento posterior e uso em modelos de aprendizado de máquina.\n",
        "\n"
      ],
      "metadata": {
        "id": "xU5BG0kiVj28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista das marcas populares a serem mantidas\n",
        "marcas_populares = [\n",
        "    \"Fiat\", \"VW - VolksWagen\", \"Chevrolet\", \"Ford\", \"Honda\", \"Renault\",\n",
        "    \"Toyota\", \"Hyundai\", \"Jeep\", \"Citroën\", \"Nissan\", \"Mitsubishi\", \"Peugeot\"\n",
        "]\n",
        "\n",
        "arquivos = sorted(os.listdir(caminho_pasta))\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for arquivo in arquivos:\n",
        "    caminho_arquivo = os.path.join(caminho_pasta, arquivo)\n",
        "    try:\n",
        "        df = pd.read_csv(caminho_arquivo, engine=\"python\", on_bad_lines=\"skip\")\n",
        "        dfs.append(df)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao ler {arquivo}: {e}\")\n",
        "\n",
        "dados = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Criação da coluna de tempo de produção (em meses)\n",
        "dados[\"timeprod\"] = (dados[\"anoref\"] - dados[\"anomod\"]) * 12 + (dados[\"mesref\"] - 1)\n",
        "\n",
        "# Seleção das colunas relevantes\n",
        "dados_final = dados[[\"marca\", \"modelo\", \"anomod\", \"comb\", \"timeprod\", \"valor\"]]\n",
        "\n",
        "# Filtro por ano e tempo de produção plausíveis\n",
        "dados_filtrados = dados_final[\n",
        "    (dados_final[\"anomod\"] >= 1980) & (dados_final[\"anomod\"] <= 2025) &\n",
        "    (dados_final[\"timeprod\"] >= 0) & (dados_final[\"timeprod\"] <= 600)\n",
        "]\n",
        "\n",
        "# Filtro pelas marcas populares\n",
        "dados_filtrados = dados_filtrados[dados_filtrados[\"marca\"].isin(marcas_populares)]\n",
        "\n",
        "# Remoção de duplicatas\n",
        "dt = dados_filtrados.drop_duplicates()\n",
        "\n",
        "print(dt)\n"
      ],
      "metadata": {
        "id": "Vy6MzYQ7jf_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.4. Preparação dos Dados para Treinamento**\n",
        "\n",
        "Nesta etapa, os dados brutos são processados e preparados para alimentar o modelo de regressão. O processo envolve:\n",
        "\n",
        "1. **Limpeza Inicial dos Dados**  \n",
        "   - Remoção de registros incompletos\n",
        "   - Separação entre features (variáveis preditoras) e target (valor a ser previsto)\n",
        "\n",
        "2. **Divisão dos Dados**  \n",
        "   - Separação em conjuntos de treino (80%) e teste (20%)\n",
        "   - Garantia de reprodutibilidade através de seed fixa\n",
        "\n",
        "3. **Transformação das Variáveis**  \n",
        "   - Categóricas: convertidas para formato numérico via codificação\n",
        "   - Numéricas: normalizadas para mesma escala\n",
        "   - Aplicação consistente das transformações em treino e teste\n",
        "\n",
        "4. **Conversão para Tensores**  \n",
        "   ```python\n",
        "   # Exemplo básico de conversão\n",
        "   dados_treino = torch.tensor(valores, dtype=torch.float32)\n",
        "   ```\n",
        "   - Adequação ao formato esperado pelo PyTorch\n",
        "   - Ajuste das dimensões dos tensores\n",
        "\n",
        "5. **Verificação Final**\n",
        "   - Confirmação das dimensões dos dados\n",
        "   - Validação dos tipos de dados\n",
        "\n",
        "Todo o pipeline garante que os dados estejam devidamente estruturados e normalizados antes do treinamento do modelo."
      ],
      "metadata": {
        "id": "kzppV7fMVAY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Separar features (X) e alvo (y)\n",
        "df = dt.dropna()\n",
        "X = df.drop('valor', axis=1)\n",
        "y = df['valor']\n",
        "\n",
        "# 2. Dividir os dados em conjuntos de treino e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Pré-processamento dos dados\n",
        "categorical_features = ['marca', 'modelo', 'comb']\n",
        "numerical_features = ['anomod', 'timeprod']\n",
        "\n",
        "# --- Processando os dados de TREINO ---\n",
        "scaler = StandardScaler()\n",
        "X_train_numerical_scaled = scaler.fit_transform(X_train[numerical_features])\n",
        "X_train_numerical = pd.DataFrame(X_train_numerical_scaled, columns=numerical_features, index=X_train.index)\n",
        "X_train_categorical = pd.get_dummies(X_train[categorical_features], drop_first=True)\n",
        "X_train_processed = pd.concat([X_train_numerical, X_train_categorical], axis=1)\n",
        "\n",
        "# --- Processando os dados de TESTE ---\n",
        "X_test_numerical_scaled = scaler.transform(X_test[numerical_features])\n",
        "X_test_numerical = pd.DataFrame(X_test_numerical_scaled, columns=numerical_features, index=X_test.index)\n",
        "X_test_categorical = pd.get_dummies(X_test[categorical_features], drop_first=True)\n",
        "X_test_processed = pd.concat([X_test_numerical, X_test_categorical], axis=1)\n",
        "\n",
        "\n",
        "X_test_processed = X_test_processed.reindex(columns=X_train_processed.columns, fill_value=0)\n",
        "\n",
        "# 4. Conversão para Tensores PyTorch\n",
        "\n",
        "# FORÇA a conversão de todos os dados para float32 ANTES de criar o tensor.\n",
        "# Isso resolve o problema de conversão do NumPy.\n",
        "# Usamos .to_numpy() que é a forma moderna e recomendada no lugar de .values\n",
        "X_train_tensor = torch.tensor(X_train_processed.astype('float32').to_numpy())\n",
        "y_train_tensor = torch.tensor(y_train.astype('float32').to_numpy()).view(-1, 1)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_processed.astype('float32').to_numpy())\n",
        "y_test_tensor = torch.tensor(y_test.astype('float32').to_numpy()).view(-1, 1)\n",
        "\n",
        "print(X_train_tensor.shape)\n",
        "print(y_train_tensor.shape)\n",
        "print(X_test_tensor.shape)\n",
        "print(y_test_tensor.shape)"
      ],
      "metadata": {
        "id": "AaSuhL3mzYVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Modelos**"
      ],
      "metadata": {
        "id": "JGTYDhvZId3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regressão Linear**"
      ],
      "metadata": {
        "id": "0Sgi7Aq0h2nO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementação do Modelo"
      ],
      "metadata": {
        "id": "ubmqTe4eiSEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(d2l.Module):\n",
        "    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n",
        "    def __init__(self, lr):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.LazyLinear(1)\n",
        "        self.net.weight.data.normal_(0, 0.01)\n",
        "        self.net.bias.data.fill_(0)"
      ],
      "metadata": {
        "id": "d7C5bmlyIf9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(LinearRegression)\n",
        "def forward(self, X):\n",
        "    return self.net(X)"
      ],
      "metadata": {
        "id": "ZzwITG59JAVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(LinearRegression)\n",
        "def loss(self, y_hat, y):\n",
        "    mse = nn.MSELoss()(y_hat, y)\n",
        "    l2 = 0.001 * torch.sum(self.net.weight.data ** 2)  # λ = 0.001\n",
        "    return mse + l2"
      ],
      "metadata": {
        "id": "cj3XVaLyJFRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@d2l.add_to_class(LinearRegression)\n",
        "def configure_optimizers(self):\n",
        "    return torch.optim.SGD(self.parameters(), self.lr)"
      ],
      "metadata": {
        "id": "2oqLeScPJHCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treino"
      ],
      "metadata": {
        "id": "eqEN3NG7iWiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instanciando o modelo\n",
        "from d2l import torch as d2l\n",
        "\n",
        "# Cria o modelo com taxa de aprendizado (por exemplo, 0.03)\n",
        "model = LinearRegression(lr=0.03)\n",
        "\n",
        "# Inicializa o trainer da D2L (gerencia o loop de treino)\n",
        "trainer = d2l.Trainer(max_epochs=80)"
      ],
      "metadata": {
        "id": "I2NNw_u0s91n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#treinando o modelo\n",
        "# Prepara o conjunto de dados como TensorDataset\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from d2l import torch as d2l\n",
        "\n",
        "# Conjunto de treino e teste\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor) # Create test dataset as well\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16) # Create test loader\n",
        "\n",
        "# Create a DataModule\n",
        "class DataModule(d2l.DataModule):\n",
        "    def __init__(self, train_loader, test_loader):\n",
        "        super().__init__()\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = test_loader # Use test_loader as validation loader for this example\n",
        "\n",
        "    def get_dataloader(self, train):\n",
        "        if train:\n",
        "            return self.train_loader\n",
        "        else:\n",
        "            return self.val_loader\n",
        "\n",
        "data = DataModule(train_loader, test_loader)\n",
        "\n",
        "# Treinamento\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "3SgrKwZGtIsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliação"
      ],
      "metadata": {
        "id": "cvwPHyx4iYki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Avaliação no conjunto de teste\n",
        "model.eval()  # Coloca o modelo em modo de avaliação\n",
        "predictions = []\n",
        "targets = []\n",
        "\n",
        "with torch.no_grad():  # Desliga o cálculo de gradiente para avaliação\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        predictions.append(y_pred)\n",
        "        targets.append(y_batch)\n",
        "\n",
        "# Concatena todos os resultados\n",
        "preds = torch.cat(predictions, dim=0)\n",
        "targets = torch.cat(targets, dim=0)\n",
        "\n",
        "# Calcula o MSE e RMSE\n",
        "mse = F.mse_loss(preds, targets).item()\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "# Calcula o MAE\n",
        "mae = F.l1_loss(preds, targets).item()\n",
        "\n",
        "print(f\"RMSE (erro quadrático médio raiz): {rmse:.2f}\")\n",
        "print(f\"MAE (erro absoluto médio): {mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "UmaIynrVe0Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualização"
      ],
      "metadata": {
        "id": "gl_AiCFvtUnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#comparar visualmente o valor real com a previsão:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Converte tensores para numpy\n",
        "y_true = y_test_tensor.view(-1).detach().numpy()\n",
        "# Collect all predictions from the test set\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        y_pred_batch = model(X_batch)\n",
        "        all_predictions.append(y_pred_batch)\n",
        "\n",
        "y_pred_np = torch.cat(all_predictions, dim=0).view(-1).detach().numpy()\n",
        "\n",
        "\n",
        "# Gráfico de dispersão\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(y_true, y_pred_np, alpha=0.5)\n",
        "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "plt.xlabel(\"Valor real\")\n",
        "plt.ylabel(\"Valor previsto\")\n",
        "plt.title(\"Regressão Linear: Valor Real vs Previsto\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IcloxFdYtbAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo MLP**"
      ],
      "metadata": {
        "id": "GtESdTcfwPsb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementação do Modelo"
      ],
      "metadata": {
        "id": "NOGHOxZHimfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica log1p para evitar problemas com valores zero\n",
        "y_train_log_tensor = torch.log1p(y_train_tensor)\n",
        "y_test_log_tensor = torch.log1p(y_test_tensor)\n",
        "\n",
        "class MLPRegressor(d2l.Module):\n",
        "    def __init__(self, lr):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(X_train_tensor.shape[1], 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def loss(self, y_hat, y):\n",
        "        return nn.MSELoss()(y_hat, y)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.net(X)"
      ],
      "metadata": {
        "id": "sDij5mvDwBSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treino"
      ],
      "metadata": {
        "id": "zIRCaPTowKn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define o modelo e o treinador\n",
        "model_mlp = MLPRegressor(lr=0.05)\n",
        "trainer_mlp = d2l.Trainer(max_epochs=100)\n",
        "\n",
        "# Criação dos datasets com log transformado\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_log_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_log_tensor)\n",
        "\n",
        "# DataLoaders para treinamento e teste\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "class DataModule(d2l.DataModule):\n",
        "    def __init__(self, train_loader, test_loader):\n",
        "        super().__init__()\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = test_loader  # using test as val\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self.train_loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return self.val_loader\n",
        "\n",
        "\n",
        "data_module = DataModule(train_loader, test_loader)\n",
        "\n",
        "trainer_mlp.fit(model_mlp, data_module)"
      ],
      "metadata": {
        "id": "9OoRsm_PwbMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predição e inversão da escala (exponencial)"
      ],
      "metadata": {
        "id": "3cOswVUawfT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Faz a predição e aplica inversa do log1p\n",
        "y_pred_log = model_mlp(X_test_tensor)\n",
        "y_pred_real = torch.expm1(y_pred_log)  # expm1 desfaz o log1p"
      ],
      "metadata": {
        "id": "7vvytREjwkxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliação (MSE real)"
      ],
      "metadata": {
        "id": "KESLhhh-wppO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar a função MAE\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mse = nn.MSELoss()(y_pred_real, y_test_tensor).item()\n",
        "print(f\"MSE (MLP com log): {mse:.2f}\")\n",
        "\n",
        "# Converter para numpy\n",
        "y_true = y_test_tensor.view(-1).detach().cpu().numpy()\n",
        "y_pred = y_pred_real.view(-1).detach().cpu().numpy()\n",
        "\n",
        "# Calcular RMSE e MAE\n",
        "rmse = mse ** 0.5\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "# Imprimir os resultados\n",
        "print(f\"RMSE (MLP com log): {rmse:.2f}\")\n",
        "print(f\"MAE (MLP com log): {mae:.2f}\")"
      ],
      "metadata": {
        "id": "upPdfDtT8_fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualização"
      ],
      "metadata": {
        "id": "OA0jqwzozBYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = y_test_tensor.view(-1).detach().numpy()\n",
        "y_pred = y_pred_real.view(-1).detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(y_true, y_pred, alpha=0.5)\n",
        "plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
        "plt.xlabel('Valor real')\n",
        "plt.ylabel('Valor previsto')\n",
        "plt.title('MLP (com log): Valor Real vs Previsto')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D3ciDOB6xsjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calcular o R² Score para MLP com log**"
      ],
      "metadata": {
        "id": "ZNhPBrMc4Axx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2_log = r2_score(y_true, y_pred_np)\n",
        "print(f\"R² Score (MLP com log): {r2_log:.4f}\")"
      ],
      "metadata": {
        "id": "v_Z9_cyj4EzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretação do R² Score\n",
        "\n",
        "O R² (coeficiente de determinação) avalia o quão bem o modelo explica a variação dos dados:\n",
        "\n",
        "- R² = 1.0 → predição perfeita\n",
        "- R² = 0.0 → modelo não tem utilidade (igual à média)\n",
        "- R² < 0.0 → modelo pior que a média\n",
        "- R² entre 0.6 e 0.8 → modelo razoável\n",
        "- R² > 0.8 → modelo excelente\n"
      ],
      "metadata": {
        "id": "bNUYmIzr4kP6"
      }
    }
  ]
}